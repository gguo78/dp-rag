---
title: "RAG with Differential Privacy"
author: Nicolas Grislain
date: "December 5, 2024"
fontsize: 12pt
papersize: a4
geometry: margin=3cm
documentclass: article
header-includes:
  - \usepackage{graphicx}
  - \usepackage{color}
colorlinks: true
linkcolor: blue
urlcolor: magenta
citecolor: red
abstract: |
  Retrieval-Augmented Generation (RAG) has emerged as the dominant technique to provide *Large Language Models* (LLM) with fresh and relevant context, mitigating the risk of hallucinations and improving the ovarall quality of responses in environments with fast moving knoledge bases.
  However, the integration of external documents into the generation process raises significant privacy concerns. Indeed, when added to a prompt, it is not possible to guarantee a reponse will not inadvertently expose confidential data, leading to potential breaches of privacy and ethical dilemmas.
  This paper explores a practical solution to this problem suitable to general knowledge extraction from personnal data.
bibliography: references.bib
---

# Introduction

Retrieval-Augmented Generation (RAG) has become a leading approach to enhance the capabilities of Large Language Models (LLMs) by supplying them with up-to-date and pertinent information. This method is particularly valuable in environments where knowledge bases are rapidly evolving, such as news websites, social media platforms, or scientific research databases. By integrating fresh context, RAG helps mitigate the risk of "hallucinations"—instances where the model generates plausible but factually incorrect information—and significantly improves the overall quality and relevance of the responses generated by the LLM.

However, incorporating external documents into the generation process introduces substantial privacy concerns. When these documents are included in the input prompt for the LLM, there is no foolproof way to ensure that the generated response will not accidentally reveal sensitive or confidential data. This potential for inadvertent data exposure can lead to serious breaches of privacy and presents significant ethical challenges. For instance, if an LLM is used in a healthcare setting and it accidentally includes patient information from an external document in its response, it could violate patient confidentiality and legal regulations.

This paper describes a practical solution aimed at addressing these privacy concerns with *Differential Privacy* (DP). The solution is based on two pillars:

* A method to collect documents related to the question in a way that does not prevent its output to be used in a DP mechanism.
* A method to use the collected documents to prompt a LLM and produce a reponse with DP guarantees.

# Related Work

A straightforward approach to *add* private knowledge to an existing LLM is to continue its training with the new knowmedge or *Fine Tune* (FT) it. This raise challenges in the case of private data as LLMs *memorize training data* (see [@shokri2017] or [@carlini2021]).
To mitigate this risk, it is possible to *redact* sensitive content prior to the FT process, but it is not reliable and require jsudgement on what should be redacted.

The current approaches to Private LLM 

A reference [@Abadi_2016]

[@yue2023synthetictextgenerationdifferential]

Private RAG

Some solutions are based on privacy preserving synthetic data generation:
[@zeng2024mitigatingprivacyissuesretrievalaugmented]

[@Ponomareva_2023]

[@lebensold2024dprdmadaptingdiffusionmodels]

[@lin2024differentiallyprivatesyntheticdata]

[@xie2024differentiallyprivatesyntheticdata]

[@tang2024privacypreservingincontextlearningdifferentially]

[@wu2023privacypreservingincontextlearninglarge]

[@hong2024dpoptmakelargelanguage]

# DP-RAG

## Overview


## Privacy Unit Preserving Document Retrieval


## Differentially Private In-Context Learning


# Evaluation


# Conclusion

