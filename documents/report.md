---
title: "RAG with Differential Privacy"
author: Nicolas Grislain
date: "December 5, 2024"
fontsize: 12pt
papersize: a4
geometry: margin=3cm
documentclass: article
header-includes:
  - \usepackage{graphicx}
  - \usepackage{color}
colorlinks: true
linkcolor: blue
urlcolor: magenta
citecolor: red
abstract: |
  Retrieval-Augmented Generation (RAG) has emerged as the dominant technique to provide *Large Language Models* (LLM) with fresh and relevant context, mitigating the risk of hallucinations and improving the ovarall quality of responses in environments with fast moving knoledge bases.
  However, the integration of external documents into the generation process raises significant privacy concerns. Indeed, when added to a prompt, it is not possible to guarantee a reponse will not inadvertently expose confidential data, leading to potential breaches of privacy and ethical dilemmas.
  This paper explores a practical solution to this problem suitable to general knowledge extraction from personnal data.
  It shows *differentially private token generation* is a viable approach to private RAG.
bibliography: references.bib
---

# Introduction

Retrieval-Augmented Generation (RAG) has become a leading approach to enhance the capabilities of Large Language Models (LLMs) by supplying them with up-to-date and pertinent information. This method is particularly valuable in environments where knowledge bases are rapidly evolving, such as news websites, social media platforms, or scientific research databases. By integrating fresh context, RAG helps mitigate the risk of "hallucinations"—instances where the model generates plausible but factually incorrect information—and significantly improves the overall quality and relevance of the responses generated by the LLM.

However, incorporating external documents into the generation process introduces substantial privacy concerns. When these documents are included in the input prompt for the LLM, there is no foolproof way to ensure that the generated response will not accidentally reveal sensitive or confidential data [@qi2024followinstructionspillbeans]. This potential for inadvertent data exposure can lead to serious breaches of privacy and presents significant ethical challenges. For instance, if an LLM is used in a healthcare setting and it accidentally includes patient information from an external document in its response, it could violate patient confidentiality and legal regulations.

This paper describes a practical solution aimed at addressing these privacy concerns with *Differential Privacy* (DP). The solution is based on two pillars:

* A method to collect documents related to the question in a way that does not prevent its output to be used in a DP mechanism.
* A method to use the collected documents to prompt a LLM and produce a reponse with DP guarantees.

The paper describes also some empirical tests and shows that *DP-RAG* is most effective in context where enough documents give elements of response.

# Related Work

In general there are 2 families of approaches to add new knowledge to an LLM. The first *Fine Tunning* (FT) and the other is *Retrieval Augmented Generation* (RAG).
In both these approaches, adding privacy can be done, through simple heuristics with human validation such as *masking* or using a systematic and principle-based approach such as *Differential Privacy*.

## Private Fine-Tuning

A straightforward approach to adding knowledge to an existing LLM is to continue its training with the new knowledge or to Fine Tune (FT) it. However, this raises challenges when dealing with private data, as LLMs tend to memorize training data.
(see [@shokri2017] or [@carlini2021]).

To mitigate this privacy risk, it is possible to redact sensitive content prior to the FT process (aka. *masking*), but this operation is not very reliable and requires judgment on what should be redacted. This is a difficult manual operation based on the perceived sensitivity of each field and how it can be used to re-identify an individual, especially when combined with other publicly available data. Overall, it is very easy to get wrong; leaning too much on the side of prudence can yield useless data, while trying to optimize utility may result in leaking sensitive information.

A solution to this problem is to leverage *Differential Privacy*, a theoretical framework enabling the computation of aggregates with formal privacy garantees (See [@dwork2014algorithmic]).

The most common approache to Private LLM FT is to use Differentially-Private-Stochastic-Gradient-Descent (DP-SGD, see [@Abadi_2016] and [@Ponomareva_2023]). DP-SGD is about clipping gradients and adding them some noise while running your ordinary SGD (or standard variants such as *Adam*, etc.). This method requires the data to be organized per *privacy unit* (typically a privacy unit will be a user). Every training example should belong to one and only one privacy unit[^1].

[^1]: Note that observations (examples) can be grouped into composite observations if one user contributes to many observations.

But, when new documents are frequently added to the private knowledge base, or when data is scarcen, FT may not be the best approach.

## Private RAG

When FT is not the best approach to adding new knowledge, DP-FT cannot help with privacy. In these cases, DP can be levreaged in different ways.
A straightforward approach to private RAG is to generate synthetic documents with differential privacy out of the private documents and retriave documents from tha synthetic documents instead of the private ones.
Another approach is to generate the LLM response in a DP way.

### Private RAG from private synthetic documents

The approach of generating synthetic documents with DP has been explored by [@zeng2024mitigatingprivacyissuesretrievalaugmented]


rain a generative model on the private data and use the trained model to generate 

[@yue2023synthetictextgenerationdifferential]

### Private generation for RAG
Private RAG

Some solutions are based on privacy preserving synthetic data generation


[@Ponomareva_2023]

[@lebensold2024dprdmadaptingdiffusionmodels]

[@lin2024differentiallyprivatesyntheticdata]

[@xie2024differentiallyprivatesyntheticdata]

[@tang2024privacypreservingincontextlearningdifferentially]

[@wu2023privacypreservingincontextlearninglarge]

In [@hong2024dpoptmakelargelanguage], the author use a technique developped in [@sordoni2023jointpromptoptimizationstacked] and [@zhou2023largelanguagemodelshumanlevel] and make it differentially private.

# DP-RAG

## Overview


## Privacy Unit Preserving Document Retrieval


## Differentially Private In-Context Learning

[@durfee2019practicaldifferentiallyprivatetopk] could be used for token selection

# Evaluation


# Conclusion

